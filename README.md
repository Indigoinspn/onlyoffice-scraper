# ONLYOFFICE Offices Scraper

> A script to automatically scrape office location data from the official ONLYOFFICE website.

---

## ğŸš€ Overview

This script uses [Playwright](https://playwright.dev/) to navigate the ONLYOFFICE contacts page, extract office details and save them to a CSV file.

The project follows clean architecture principles:

- Separation of concerns
- Comprehensive testing (unit, integration, E2E)
- Full JSDoc-based type safety
- Automated CI/CD pipeline

---

## ğŸ› ï¸ Technologies & Tools

| Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚     | ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ                           |
| -------------- | ------------------------------------ |
| [Node.js]      | Runtime environment                  |
| [Playwright]   | Browser automation (Firefox)         |
| [Jest]         | Unit and integration tests           |
| [csv-writer]   | CSV file generation                  |
| [JSDoc]        | Type safety and inline documentation |
| GitHub Actions | CI/CD automation                     |

---

## ğŸ—‚ï¸ Project Structure

```bash
ssrc/
â”œâ”€â”€ config/ # Configuration files
â”‚ â”œâ”€â”€ index.js # Main config export
â”‚ â”œâ”€â”€ messages.js # Log and error messages
â”‚ â”œâ”€â”€ selectors.js # CSS selectors for DOM elements
â”‚ â””â”€â”€ timeouts.js # Timeout values
â”œâ”€â”€ services/ # Business logic services
â”‚ â””â”€â”€ saveOfficesToCsv.js
â”œâ”€â”€ utils/ # Helper utilities
â”‚ â”œâ”€â”€ ensureDir.js # Ensure directory exists
â”‚ â”œâ”€â”€ retry.js # Retry function for flaky operations
â”‚ â”œâ”€â”€ sortOfficesByLocality.js # Sort offices by locality
â”‚ â””â”€â”€ cli.js # CLI entry point (main script)
â””â”€â”€ scraper.js # Core scraping logic (uses page.$$eval)

tests/
â”œâ”€â”€ unit/ # Unit tests (no browser)
â”‚ â”œâ”€â”€ ensureDir.test.js
â”‚ â”œâ”€â”€ retry.test.js
â”‚ â””â”€â”€ sortOfficesByLocality.test.js
â”œâ”€â”€ integration/ # Integration tests (DOM parsing logic)
â”‚ â””â”€â”€ extractOffices.test.js
â””â”€â”€ e2e/ # End-to-end tests (real browser + live site)
â””â”€â”€ scraper.e2e.test.js

.github/workflows/ci.yml # CI/CD pipeline configuration

```

---

## ğŸ§ª Testing Strategy

ğŸ”¹ Unit Tests (fast & reliable)

Run on every commit and PR:

```bash
npm test
```

ğŸ”¹ Integration Tests

Validate data extraction logic without a browser:

```bash
npm run test:integration
```

ğŸ”¹ E2E Tests (real browser)

Execute only on merge to master:

```bash
npm run test:e2e

// Run all tests:

npm run test:all
```

## ğŸ”„ CI/CD Pipeline (GitHub Actions)

Automated test execution on push to master or pull request:

- âœ… Unit + Integration tests â†’ on every PR
- âš ï¸ E2E tests â†’ only on merge to master
- ğŸ“ Artifacts (screenshots, CSV) â†’ saved on E2E failure

  > Config file: .github/workflows/ci.yml

## ğŸš€ Getting Started

1. Install dependencies

```bash
npm ci
```

2. Run the scraper (specify output CSV path)

```bash
node src/cli.js data/offices.csv

// or
npm run scrape data/offices.csv
```

> ğŸ“ The data/ directory will be created automatically if it doesnâ€™t exist.

ğŸ“ Sample Output

```bash
â¡ï¸  Starting scraper for: https://www.onlyoffice.com
ğŸŒ Target URL: https://www.onlyoffice.com
âœ…  Page loaded: https://www.onlyoffice.com/
ğŸ“–  Opening Resources menu...
ğŸ“  Navigating to Contacts...
â³  Waiting for office data to load...
â™»ï¸   Extracting office data...
âœï¸   Saving data to file: data/offices.csv
âœ…  Successfully scraped 9 offices.
```

## Error Handling & Validation

- Configuration validation (BASE_URL, SELECTORS)
- Automatic retry logic for transient failures
- Required field validation
- Debug screenshots on error (DEBUG_MODE=true)
- Clear, actionable error messages
